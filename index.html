<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models </title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block"> Jingming Liu<sup>*</sup>,</span>
              <span class="author-block"> Yumeng Li<sup>*</sup>,</span>
              <span class="author-block"> <sup></sup>Boyuan Xiao,</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"> <sup></sup>Yichang Jian,</span>
              <span class="author-block"> <sup></sup>Ziang Qin,</span>
              <span class="author-block"> <sup></sup>Tianjia Shao,</span>
              <span class="author-block"> <sup></sup>Yaoxiang Ding,</span>
              <span class="author-block"> <sup></sup>Kun Zhou</span>
            </div>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Zhejiang University</span>
          </div>
          <div class="is-size-7 publication-authors">
            <span class="author-block"><sup>*</sup>Denotes Equal Contribution</span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
          <figure>
            <img src="./figure/teaser.jpg" alt="Our autonomous imagination method empowers advanced MLLMs to engage in iterative
                                                imaginative reasoning, enabling them to address previously unsolvable tasks without additional
                                                training or fine-tuning">
            <figcaption>Our autonomous imagination method empowers advanced MLLMs to engage in iterative
            imaginative reasoning, enabling them to address previously unsolvable tasks without additional
            training or fine-tuning</figcaption>
          </figure>
      <h2 class="subtitle has-text-centered">
        
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            There have been recent efforts to extend the Chain-of-Thought (CoT) paradigm to Multimodal Large Language Models (MLLMs) by finding visual clues in the input scene, advancing the visual reasoning ability of MLLMs. However, current approaches are specially designed for the tasks where clue finding plays a major role in the whole reasoning process, leading to the difficulty in handling complex visual scenes where clue finding does not actually simplify the whole reasoning task. To deal with this challenge, we propose a new visual reasoning paradigm enabling MLLMs to autonomously modify the input scene to new ones based on its reasoning status, such that CoT is reformulated as conducting simple closed-loop decision-making and reasoning steps under a sequence of “imagined” visual scenes, leading to natural and general CoT construction. To implement this paradigm, we introduce a novel plug-and-play “imagination space”, where MLLMs conduct visual modifications through operations like focus, ignore, and transform based on their native reasoning ability without specific training. We validate our approach through a benchmark spanning dense counting, simple jigsaw puzzle solving, and object placement, challenging the reasoning ability beyond clue finding. The results verify that while existing techniques fall short, our approach enables MLLMs to effectively reason step by step through autonomous imagination.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            The imagination space begins with
            an unstructured input scene and undergoes an iterative reasoning process. In each cycle, MLLMs first
            perceive the current state of the imagination space, select an operation to apply, and then reassess the
            updated imagination space. Upon completing this reasoning sequence, MLLMs generate an answer
            based on the cumulative context of the process and the final state of the imagination space.
          </p>
          <figure>
            <img src="./figure/method.jpg" alt="An overview of our autonomous imagination method">
            <figcaption>An overview of our autonomous imagination method</figcaption>
          </figure>
        </div>
      </div>
    </div>
    <!--/ Method.. -->
    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="jigsaw puzzle visualization.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
      ...
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="..." class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed
            under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
